\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
    citecolor=green,
}

% Title information
\title{
    \textbf{A Study of Embeddings, LLMs, and RAG Methods}\\[0.3cm]
    \large Information Retrieval -- Project Stage 1
}

\author{
    Patrascu Adrian\textsuperscript{1} \and
    Modiga Miriam\textsuperscript{1} \and
    Toderian Vitalii\textsuperscript{1}\\[0.3cm]
    \textsuperscript{1}Faculty of Automatic Control and Computer Science\\
    Politehnica University of Bucharest
}

\date{November 2024}

\begin{document}

\maketitle

%==============================================================================
\section{Project Description}
%==============================================================================

This project aims to benchmark and compare two prominent Retrieval-Augmented Generation (RAG) methods: \textbf{ColBERT} and \textbf{FAISS}. RAG systems enhance Large Language Model responses by retrieving relevant documents from a knowledge base before generating answers.

\subsection{Objectives}

\begin{itemize}[noitemsep]
    \item Run 2-3 Information Retrieval benchmarks
    \item Compare 3-5 sparse vs dense embedding methods
    \item Evaluate 3-5 open-source LLMs for generation
    \item Implement conversational tests using LangGraph
    \item Analyze trade-offs between accuracy, speed, and memory usage
\end{itemize}

\subsection{Scope}

We will evaluate:
\begin{itemize}[noitemsep]
    \item \textbf{Retrieval Methods}: ColBERT (late interaction) vs FAISS (approximate nearest neighbor)
    \item \textbf{Embedders}: BM25, SPLADE (sparse); MiniLM, BGE, E5 (dense)
    \item \textbf{LLMs}: Llama 2, Mistral 7B, Phi-2, Zephyr 7B
    \item \textbf{Benchmarks}: MS MARCO, Natural Questions
\end{itemize}

%==============================================================================
\section{State-of-the-Art}
%==============================================================================

\subsection{ColBERT -- Late Interaction Retrieval}

ColBERT \cite{khattab2020colbert} introduces a "late interaction" architecture that independently encodes queries and documents using BERT, then computes relevance via MaxSim operation:

\begin{equation}
S_{q,d} = \sum_{i} \max_{j} E_{q_i} \cdot E_{d_j}^T
\end{equation}

\textbf{Key advantages}: Pre-computed document embeddings, token-level matching, scalable to large collections.

\textbf{Implementations}:
\begin{itemize}[noitemsep]
    \item Official: \url{https://github.com/stanford-futuredata/ColBERT}
    \item RAGatouille: \url{https://github.com/AnswerDotAI/RAGatouille} (used in this project)
\end{itemize}

\subsection{FAISS -- Similarity Search at Scale}

FAISS \cite{johnson2017billion} is Facebook's library for efficient similarity search in high-dimensional spaces. It supports multiple index types:

\begin{itemize}[noitemsep]
    \item \textbf{Flat}: Exact search (baseline)
    \item \textbf{IVF}: Inverted file index for faster search
    \item \textbf{HNSW}: Graph-based approximate search
    \item \textbf{PQ}: Product quantization for compression
\end{itemize}

\textbf{Implementation}: \url{https://github.com/facebookresearch/faiss}

\subsection{Related Work}

\begin{itemize}[noitemsep]
    \item \textbf{DPR} \cite{karpukhin2020dense}: Dense Passage Retrieval for open-domain QA
    \item \textbf{SPLADE} \cite{formal2021splade}: Sparse lexical and expansion model
    \item \textbf{BEIR} \cite{thakur2021beir}: Benchmark for zero-shot IR evaluation
    \item \textbf{LangChain/LangGraph}: Frameworks for building LLM applications
\end{itemize}

%==============================================================================
\section{Technologies}
%==============================================================================

\subsection{Core Stack}

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Technology} \\
\midrule
Language & Python 3.10+ \\
Deep Learning & PyTorch 2.0+ \\
LLM Framework & Transformers, vLLM \\
RAG Pipeline & LangChain, LangGraph \\
ColBERT & RAGatouille (colbert-ir/colbertv2.0) \\
Vector Search & FAISS (CPU/GPU), PGVector \\
Embeddings & Sentence-Transformers \\
Evaluation & datasets, ranx \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Models}

\textbf{Embedders}:
\begin{itemize}[noitemsep]
    \item Sparse: BM25, SPLADE
    \item Dense: all-MiniLM-L6-v2, bge-base-en, e5-base-v2
\end{itemize}

\textbf{LLMs}: Llama 2 7B, Mistral 7B, Phi-2, Zephyr 7B

%==============================================================================
\section{System Architecture}
%==============================================================================

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.2cm,
    box/.style={rectangle, draw, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\small},
    arrow/.style={->, thick}
]

% Nodes
\node[box] (query) {User Query};
\node[box, below=of query] (embed) {Query Embedding};
\node[box, below left=1cm and 0.5cm of embed] (colbert) {ColBERT\\Retriever};
\node[box, below right=1cm and 0.5cm of embed] (faiss) {FAISS\\Retriever};
\node[box, below=2.5cm of embed] (docs) {Retrieved\\Documents};
\node[box, below=of docs] (llm) {LLM\\Generation};
\node[box, below=of llm] (response) {Response};

% Document store
\node[box, right=2cm of faiss] (store) {Document\\Index};

% Arrows
\draw[arrow] (query) -- (embed);
\draw[arrow] (embed) -- (colbert);
\draw[arrow] (embed) -- (faiss);
\draw[arrow] (colbert) -- (docs);
\draw[arrow] (faiss) -- (docs);
\draw[arrow] (docs) -- (llm);
\draw[arrow] (llm) -- (response);
\draw[arrow] (store) -- (faiss);
\draw[arrow] (store) -- (colbert);

\end{tikzpicture}
\caption{RAG System Architecture}
\end{figure}

\subsection{Pipeline Components}

\begin{enumerate}[noitemsep]
    \item \textbf{Ingestion}: Load documents, chunk text, generate embeddings
    \item \textbf{Indexing}: Build ColBERT index or FAISS index
    \item \textbf{Retrieval}: Search for relevant passages given query
    \item \textbf{Generation}: Use LLM to generate answer from context
    \item \textbf{Evaluation}: Compute metrics (MRR, Recall, NDCG)
\end{enumerate}

\subsection{LangGraph Integration}

LangGraph orchestrates the conversational RAG pipeline:
\begin{itemize}[noitemsep]
    \item State management across conversation turns
    \item Conditional routing between retrievers
    \item History-aware query reformulation
\end{itemize}

\subsection{Implementation Status}

\subsubsection{Current Progress}

As of Milestone 1, the following components have been implemented on the \texttt{miriam/colbert-integration} branch:

\begin{itemize}[noitemsep]
    \item \textbf{ColBERT Retriever Module}: Complete implementation using RAGatouille library with the official ColBERTv2.0 model (\texttt{colbert-ir/colbertv2.0})
    \item \textbf{Core Functionality}: Document indexing with late interaction search, returning ranked results with relevance scores
    \item \textbf{Test Suite}: Validation scripts demonstrating indexing and retrieval on sample datasets
    \item \textbf{Vector Database}: PGVector backend for FAISS integration
    \item \textbf{RAG Pipeline}: LangChain-based retrieval-augmented generation with vLLM
\end{itemize}

\subsubsection{Technical Implementation Details}

\textbf{ColBERT Configuration:}
\begin{itemize}[noitemsep]
    \item Model: \texttt{colbert-ir/colbertv2.0}
    \item Library: RAGatouille (Python wrapper)
    \item Max document length: 512 tokens (configurable)
    \item Index storage: Local filesystem (\texttt{.ragatouille} directory)
    \item LangChain integration: Full compatibility with Document objects
\end{itemize}

\textbf{Code Organization:}
\begin{itemize}[noitemsep]
    \item \texttt{backend/colbert\_retriever.py}: Core retriever implementation (101 lines)
    \item \texttt{backend/test\_colbert.py}: Integration testing
    \item \texttt{backend/rag\_components.py}: RAG pipeline orchestration
    \item \texttt{backend/multi\_embedder\_manager.py}: Multi-model embedding support
\end{itemize}

\subsubsection{Ongoing Development}

The following features are currently under development:

\begin{itemize}[noitemsep]
    \item Integration of ColBERT retriever with main FastAPI application
    \item API endpoints for ColBERT document indexing and search
    \item Reranking functionality for hybrid retrieval approaches
    \item Comparative benchmarking framework between ColBERT and FAISS
    \item LangGraph integration for conversational RAG with multiple retrieval strategies
\end{itemize}

%==============================================================================
\section{Potential Challenges}
%==============================================================================

\subsection{Technical Challenges}

\begin{enumerate}
    \item \textbf{Memory constraints}: ColBERT indexes can be large (storing per-token embeddings). Solution: Use compression, quantization.

    \item \textbf{GPU requirements}: Running multiple LLMs requires significant VRAM. Solution: Use quantized models (4-bit), CPU offloading.

    \item \textbf{Indexing time}: Building ColBERT indexes is slower than FAISS. Solution: Pre-build indexes, use batch processing.

    \item \textbf{Benchmark consistency}: Ensuring fair comparison across different methods. Solution: Standardized evaluation scripts, same hardware.
\end{enumerate}

\subsection{Methodological Challenges}

\begin{enumerate}
    \item \textbf{Hyperparameter tuning}: Each method has different optimal settings (chunk size, top-k, temperature). Solution: Grid search on validation set.

    \item \textbf{Metric selection}: Different metrics favor different systems. Solution: Report multiple metrics (MRR, Recall, latency).

    \item \textbf{LLM variability}: Generation quality varies with prompts. Solution: Use consistent prompt templates.
\end{enumerate}

\subsection{Expected Trade-offs}

\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Aspect} & \textbf{ColBERT} & \textbf{FAISS} \\
\midrule
Retrieval accuracy & Higher & Lower \\
Query latency & Higher & Lower \\
Index size & Larger & Smaller \\
Setup complexity & More complex & Simpler \\
\bottomrule
\end{tabular}
\caption{Expected Trade-offs}
\end{table}

%==============================================================================
\section{Timeline and Deliverables}
%==============================================================================

\subsection{Project Stages}

\begin{enumerate}[noitemsep]
    \item \textbf{Stage 1} (Current): Project description, architecture, technology selection
    \item \textbf{Stage 2}: Implementation of retrieval pipelines
    \item \textbf{Stage 3}: Benchmark evaluation and results analysis
    \item \textbf{Final}: Complete report with findings and recommendations
\end{enumerate}

\subsection{Milestone 1 Achievements}

\begin{itemize}[noitemsep]
    \item \textbf{Architecture Design}: Complete system architecture for dual-retriever RAG system
    \item \textbf{Technology Stack}: Selection and validation of core technologies (RAGatouille, FAISS, vLLM, LangChain)
    \item \textbf{ColBERT Implementation}: Working ColBERT retriever module with indexing and search capabilities
    \item \textbf{Development Environment}: Dockerized setup with PostgreSQL, vLLM, and frontend
    \item \textbf{Test Framework}: Unit tests and integration tests for retrieval components
\end{itemize}

\subsection{Next Steps for Stage 2}

\begin{itemize}[noitemsep]
    \item Merge ColBERT integration branch into main codebase
    \item Implement unified API endpoints for both retrieval methods
    \item Develop comparative benchmarking scripts using MS MARCO dataset
    \item Create evaluation metrics dashboard
    \item Integrate multiple embedding models (BM25, SPLADE, MiniLM, BGE, E5)
    \item Conduct initial performance testing on target hardware
\end{itemize}

%==============================================================================
% References
%==============================================================================

\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
