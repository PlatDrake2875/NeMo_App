\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
    citecolor=green,
}

% Title information
\title{
    \textbf{A Study of Embeddings, LLMs, and RAG Methods}\\[0.3cm]
    \large Information Retrieval -- Project Stage 1
}

\author{
    Patrascu Adrian\textsuperscript{1} \and
    Modiga Miriam\textsuperscript{1} \and
    Toderian Vitalii\textsuperscript{1}\\[0.3cm]
    \textsuperscript{1}Faculty of Automatic Control and Computer Science\\
    Politehnica University of Bucharest
}

\date{November 2024}

\begin{document}

\maketitle

%==============================================================================
\section{Project Description}
%==============================================================================

This project aims to benchmark and compare two prominent Retrieval-Augmented Generation (RAG) methods: \textbf{ColBERT} and \textbf{FAISS}. RAG systems enhance Large Language Model responses by retrieving relevant documents from a knowledge base before generating answers.

\subsection{Objectives}

\begin{itemize}[noitemsep]
    \item Run 2-3 Information Retrieval benchmarks
    \item Compare 3-5 sparse vs dense embedding methods
    \item Evaluate 3-5 open-source LLMs for generation
    \item Implement conversational tests using LangGraph
    \item Analyze trade-offs between accuracy, speed, and memory usage
\end{itemize}

\subsection{Scope}

We will evaluate:
\begin{itemize}[noitemsep]
    \item \textbf{Retrieval Methods}: ColBERT (late interaction) vs FAISS (approximate nearest neighbor)
    \item \textbf{Embedders}: BM25, SPLADE (sparse); MiniLM, BGE, E5 (dense)
    \item \textbf{LLMs}: Llama 2, Mistral 7B, Phi-2, Zephyr 7B
    \item \textbf{Benchmarks}: MS MARCO, Natural Questions
\end{itemize}

%==============================================================================
\section{State-of-the-Art}
%==============================================================================

\subsection{ColBERT -- Late Interaction Retrieval}

ColBERT \cite{khattab2020colbert} introduces a "late interaction" architecture that independently encodes queries and documents using BERT, then computes relevance via MaxSim operation:

\begin{equation}
S_{q,d} = \sum_{i} \max_{j} E_{q_i} \cdot E_{d_j}^T
\end{equation}

\textbf{Key advantages}: Pre-computed document embeddings, token-level matching, scalable to large collections.

\textbf{Implementations}:
\begin{itemize}[noitemsep]
    \item Official: \url{https://github.com/stanford-futuredata/ColBERT}
    \item RAGatouille: \url{https://github.com/AnswerDotAI/RAGatouille}
\end{itemize}

\subsection{FAISS -- Similarity Search at Scale}

FAISS \cite{johnson2017billion} is Facebook's library for efficient similarity search in high-dimensional spaces. It supports multiple index types:

\begin{itemize}[noitemsep]
    \item \textbf{Flat}: Exact search (baseline)
    \item \textbf{IVF}: Inverted file index for faster search
    \item \textbf{HNSW}: Graph-based approximate search
    \item \textbf{PQ}: Product quantization for compression
\end{itemize}

\textbf{Implementation}: \url{https://github.com/facebookresearch/faiss}

\subsection{Related Work}

\begin{itemize}[noitemsep]
    \item \textbf{DPR} \cite{karpukhin2020dense}: Dense Passage Retrieval for open-domain QA
    \item \textbf{SPLADE} \cite{formal2021splade}: Sparse lexical and expansion model
    \item \textbf{BEIR} \cite{thakur2021beir}: Benchmark for zero-shot IR evaluation
    \item \textbf{LangChain/LangGraph}: Frameworks for building LLM applications
\end{itemize}

%==============================================================================
\section{Technologies}
%==============================================================================

\subsection{Core Stack}

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Technology} \\
\midrule
Language & Python 3.10+ \\
Deep Learning & PyTorch 2.0+ \\
LLM Framework & Transformers, vLLM \\
RAG Pipeline & LangChain, LangGraph \\
ColBERT & RAGatouille \\
Vector Search & FAISS (CPU/GPU) \\
Embeddings & Sentence-Transformers \\
Evaluation & datasets, ranx \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Models}

\textbf{Embedders}:
\begin{itemize}[noitemsep]
    \item Sparse: BM25, SPLADE
    \item Dense: all-MiniLM-L6-v2, bge-base-en, e5-base-v2
\end{itemize}

\textbf{LLMs}: Llama 2 7B, Mistral 7B, Phi-2, Zephyr 7B

%==============================================================================
\section{System Architecture}
%==============================================================================

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.2cm,
    box/.style={rectangle, draw, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\small},
    arrow/.style={->, thick}
]

% Nodes
\node[box] (query) {User Query};
\node[box, below=of query] (embed) {Query Embedding};
\node[box, below left=1cm and 0.5cm of embed] (colbert) {ColBERT\\Retriever};
\node[box, below right=1cm and 0.5cm of embed] (faiss) {FAISS\\Retriever};
\node[box, below=2.5cm of embed] (docs) {Retrieved\\Documents};
\node[box, below=of docs] (llm) {LLM\\Generation};
\node[box, below=of llm] (response) {Response};

% Document store
\node[box, right=2cm of faiss] (store) {Document\\Index};

% Arrows
\draw[arrow] (query) -- (embed);
\draw[arrow] (embed) -- (colbert);
\draw[arrow] (embed) -- (faiss);
\draw[arrow] (colbert) -- (docs);
\draw[arrow] (faiss) -- (docs);
\draw[arrow] (docs) -- (llm);
\draw[arrow] (llm) -- (response);
\draw[arrow] (store) -- (faiss);
\draw[arrow] (store) -- (colbert);

\end{tikzpicture}
\caption{RAG System Architecture}
\end{figure}

\subsection{Pipeline Components}

\begin{enumerate}[noitemsep]
    \item \textbf{Ingestion}: Load documents, chunk text, generate embeddings
    \item \textbf{Indexing}: Build ColBERT index or FAISS index
    \item \textbf{Retrieval}: Search for relevant passages given query
    \item \textbf{Generation}: Use LLM to generate answer from context
    \item \textbf{Evaluation}: Compute metrics (MRR, Recall, NDCG)
\end{enumerate}

\subsection{LangGraph Integration}

LangGraph orchestrates the conversational RAG pipeline:
\begin{itemize}[noitemsep]
    \item State management across conversation turns
    \item Conditional routing between retrievers
    \item History-aware query reformulation
\end{itemize}

%==============================================================================
\section{Potential Challenges}
%==============================================================================

\subsection{Technical Challenges}

\begin{enumerate}
    \item \textbf{Memory constraints}: ColBERT indexes can be large (storing per-token embeddings). Solution: Use compression, quantization.

    \item \textbf{GPU requirements}: Running multiple LLMs requires significant VRAM. Solution: Use quantized models (4-bit), CPU offloading.

    \item \textbf{Indexing time}: Building ColBERT indexes is slower than FAISS. Solution: Pre-build indexes, use batch processing.

    \item \textbf{Benchmark consistency}: Ensuring fair comparison across different methods. Solution: Standardized evaluation scripts, same hardware.
\end{enumerate}

\subsection{Methodological Challenges}

\begin{enumerate}
    \item \textbf{Hyperparameter tuning}: Each method has different optimal settings (chunk size, top-k, temperature). Solution: Grid search on validation set.

    \item \textbf{Metric selection}: Different metrics favor different systems. Solution: Report multiple metrics (MRR, Recall, latency).

    \item \textbf{LLM variability}: Generation quality varies with prompts. Solution: Use consistent prompt templates.
\end{enumerate}

\subsection{Expected Trade-offs}

\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Aspect} & \textbf{ColBERT} & \textbf{FAISS} \\
\midrule
Retrieval accuracy & Higher & Lower \\
Query latency & Higher & Lower \\
Index size & Larger & Smaller \\
Setup complexity & More complex & Simpler \\
\bottomrule
\end{tabular}
\caption{Expected Trade-offs}
\end{table}

%==============================================================================
\section{Timeline and Deliverables}
%==============================================================================

\begin{enumerate}[noitemsep]
    \item \textbf{Stage 1} (Current): Project description, architecture, technology selection
    \item \textbf{Stage 2}: Implementation of retrieval pipelines
    \item \textbf{Stage 3}: Benchmark evaluation and results analysis
    \item \textbf{Final}: Complete report with findings and recommendations
\end{enumerate}

%==============================================================================
% References
%==============================================================================

\begin{thebibliography}{9}
\small

\bibitem{khattab2020colbert}
O. Khattab and M. Zaharia, ``ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT,'' \textit{SIGIR}, 2020. \url{https://arxiv.org/abs/2004.12832}

\bibitem{johnson2017billion}
J. Johnson, M. Douze, and H. JÃ©gou, ``Billion-scale similarity search with GPUs,'' \textit{IEEE Trans. Big Data}, 2017. \url{https://arxiv.org/abs/1702.08734}

\bibitem{karpukhin2020dense}
V. Karpukhin et al., ``Dense Passage Retrieval for Open-Domain Question Answering,'' \textit{EMNLP}, 2020.

\bibitem{formal2021splade}
T. Formal et al., ``SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking,'' \textit{SIGIR}, 2021.

\bibitem{thakur2021beir}
N. Thakur et al., ``BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models,'' \textit{NeurIPS}, 2021.

\end{thebibliography}

\end{document}
